{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Three Layer Artificial Neural Network for Pattern Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, yhat):\n",
    "    yloss = -1 * (((1-y) * np.log(1-yhat)) + (y * np.log(yhat)))\n",
    "    return yloss   \n",
    "\n",
    "def cost(Y, Yhat):\n",
    "    Jcost = np.sum(loss(Y, Yhat))/Y.shape[0]\n",
    "    return Jcost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_activation(yhat):\n",
    "    return yhat\n",
    "unit_activation = np.vectorize(unit_activation)\n",
    "\n",
    "def sigmoid_activation(yhat):\n",
    "    return 1/(1+np.exp(-yhat))\n",
    "sigmoid_activation = np.vectorize(sigmoid_activation)\n",
    "\n",
    "def relu_activation(yhat):\n",
    "    return yhat * (yhat >= 0)\n",
    "relu_activation = np.vectorize(relu_activation)\n",
    "\n",
    "def relu_derivative(yhat):\n",
    "    return 1 * (yhat >= 0)\n",
    "relu_derivative = np.vectorize(relu_derivative)\n",
    "\n",
    "def tanh_activation(yhat):\n",
    "    return (np.exp(yhat)-np.exp(-yhat))/(np.exp(yhat)+np.exp(-yhat))\n",
    "tanh_activation = np.vectorize(tanh_activation)\n",
    "\n",
    "def tanh_derivative(tanh):\n",
    "    return 1-np.square(tanh)\n",
    "tanh_derivative = np.vectorize(tanh_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Matrices for Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  2\n",
      "Number of Training Examples:  320\n",
      "Number of Test Examples:  80\n"
     ]
    }
   ],
   "source": [
    "X = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[0:320,0:2].T \n",
    "Y = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[0:320,2:3].T \n",
    "\n",
    "Xt = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[320:400,0:2].T \n",
    "Yt = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[320:400,2:3].T \n",
    "Yb = np.genfromtxt('pattern_rand.csv',delimiter=',', skip_header=True)[:,2:3].T \n",
    "    \n",
    "n = X.shape[0]\n",
    "m = X.shape[1]\n",
    "t = Xt.shape[1]\n",
    "\n",
    "print(\"Number of Features: \", n)\n",
    "print(\"Number of Training Examples: \", m)\n",
    "print(\"Number of Test Examples: \", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Running Forward Propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterations\n",
    "h = 1000\n",
    "alpha = 0.05\n",
    "\n",
    "# Initialization of J and Yhat\n",
    "J = np.zeros((1,h))\n",
    "cost_sample = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# IL0: Number of inputs to Layer 0\n",
    "# NL0: Number of neurons in Layer 0\n",
    "IL0 = n\n",
    "NL0 = n\n",
    "# The first layer is input layer so both number of inputs \n",
    "# and number of neurons are equal to number of features\n",
    "\n",
    "# IL1: Number of inputs to Layer 1\n",
    "# NL1: Number of neurons in Layer 1\n",
    "IL1 = NL0\n",
    "NL1 = 4\n",
    "# As there are n neurons in the previous layer, the number\n",
    "# of inputs to Layer 1 is equal to n. Number of neurons\n",
    "# may be any. \n",
    "\n",
    "# IL2: Number of inputs to Layer 2\n",
    "# NL2: Number of neurons in Layer 2\n",
    "IL2 = NL1\n",
    "NL2 = 1\n",
    "# Number of inputs in Layer 2 equals to number of neurons in\n",
    "# the previous layer (one input coming from each neuron). As \n",
    "# this is binary classification with a single output, the number \n",
    "# of neurons equals 1. \n",
    "\n",
    "\n",
    "# Layer 0 is input layer so W0 is identity matrix while B0 is zero matrix.\n",
    "# Layer 0 in a way simply reproduces its inputs at the output\n",
    "B0 = np.zeros((NL0, 1))\n",
    "W0 = np.identity((NL0))\n",
    "\n",
    "# Layer 1 is a passthrough layer. As number of inputs to the layer equals\n",
    "# 2 and number of neurons are 4, the size of W1 will be 4 x2. To make this\n",
    "# layer a passthrough layer, the 4 x 2 W1 matrix needs to consist of a \n",
    "# a 2 x 2 identity matrix and a 2 x 2 zero matrix. Also B1 is a zero matrix\n",
    "B1 = np.zeros((NL1,1))\n",
    "W1 = np.array([[1, 0],[0, 1],[0, 0],[0, 0]])\n",
    "\n",
    "\n",
    "# Layer 2 is a simple Logistic Regression Unit. W2 and B2 are intialized as ones.\n",
    "B2 = np.ones((NL2,1))\n",
    "W2 = np.ones((NL2,IL2))\n",
    "\n",
    "# We now runs a loop for performing forward- and backpropagation\n",
    "for g in range(h):\n",
    "    # Forward propagation \n",
    "    X0 = X\n",
    "    G0 = np.matmul(W0,X0) + B0\n",
    "    H0 = unit_activation(G0)\n",
    "    X1 = H0\n",
    "    G1 = np.matmul(W1,X1) + B1\n",
    "    H1 = relu_activation(G1)\n",
    "    X2 = G1\n",
    "    G2 = np.matmul(W2,X2) + B2\n",
    "    H2 = sigmoid_activation(G2)\n",
    "    Yhat = H2\n",
    "\n",
    "    # Determine Cost\n",
    "    J[0,g] = cost(Y, Yhat)/m\n",
    "    if (g+1) % 100 == 0:\n",
    "        cost_sample.append(J[0,g])\n",
    "    \n",
    "    # Backpropagation for layer 2\n",
    "    dJdG2 = Yhat - Y\n",
    "    dJdB2 =  np.sum(dJdG2, axis=1,keepdims=True)/m \n",
    "    dJdW2 = np.matmul(dJdG2, X2.T)/m\n",
    "    B2 = B2 - alpha * dJdB2\n",
    "    W2 = W2 - alpha * dJdW2\n",
    "    \n",
    "    # Backpropagation for layer 1\n",
    "    dG1dW1 = np.multiply(np.matmul(W2.T, dJdG2), relu_derivative(H1))\n",
    "    dJdB1 = np.sum(dG1dW1)/m\n",
    "    dJdW1 = np.matmul(dG1dW1, X1.T)/m\n",
    "    B1 = B1 - alpha * dJdB1\n",
    "    W1 = W1 - alpha * dJdW1\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the change in cost function against all 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(J[0, 0:h-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the determined model parameters to predict the output for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now run forward propagation using the Weights and Biases created by training\n",
    "\n",
    "# Using the Testing data as input -- Xt\n",
    "X0 = Xt\n",
    "G0 = np.matmul(W0,X0) + B0\n",
    "H0 = unit_activation(G0)\n",
    "X1 = H0\n",
    "G1 = np.matmul(W1,X1) + B1\n",
    "H1 = relu_activation(G1)\n",
    "X2 = G1\n",
    "G2 = np.matmul(W2,X2) + B2\n",
    "H2 = sigmoid_activation(G2)\n",
    "Yhat = H2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the predicted output and actual output for the test data and determine percent acccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Round the Yhat values to 1 or 0\n",
    "Yt_hat = np.rint(Yhat)\n",
    "\n",
    "# Compare the Yhat values to the Yt values for the test data\n",
    "Yt_compare = np.equal(Yt, Yt_hat)\n",
    "\n",
    "# Compute percent accuracy\n",
    "Yt_accuracy = np.mean(Yt_compare)*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost_sample list is:\n",
      " [0.6636268397341352, 0.663626839734135, 0.663626839734135, 0.663626839734135, 0.663626839734135, 0.663626839734135, 0.663626839734135, 0.663626839734135, 0.663626839734135, 0.663626839734135]\n",
      "\n",
      "\n",
      "The accuracy is: 45.0%\n",
      "\n",
      "\n",
      "W1:\n",
      " [[ 0.80682416 -0.25819222]\n",
      " [-0.18097462  0.75678346]\n",
      " [-0.21757242 -0.28650536]\n",
      " [-0.21757242 -0.28650536]]\n",
      "B1:\n",
      " [[-0.24893055]\n",
      " [-0.24893055]\n",
      " [-0.24893055]\n",
      " [-0.24893055]]\n",
      "W2:\n",
      " [[0.84890697 0.77784511 1.06536155 1.06536155]]\n",
      "B2:\n",
      " [[0.93456137]]\n"
     ]
    }
   ],
   "source": [
    "# Print the cost sample\n",
    "print(\"The cost_sample list is:\\n\", cost_sample)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print out the accuracy using the mean of True/False values\n",
    "print(\"The accuracy is: {}%\".format(Yt_accuracy))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the model parameters\n",
    "print(\"W1:\\n\", W1)\n",
    "print(\"B1:\\n\", B1)\n",
    "print(\"W2:\\n\", W2)\n",
    "print(\"B2:\\n\", B2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
