{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Assignment\n",
    "\n",
    "This notebook needs the <b>AUTO_BIOS</b> parameter updated BEFORE it can be run using \"Run All\".\n",
    "\n",
    "<b>Note</b> - Since I intend to use the Sklearn, nltk and other available libraries for my projects, I need to learn how to run Pipelines. So I'm going to follow the examples from the 'Applied Text Analysis with Python' (ATAP) textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter needs to be updated to point to the HTML of the Class Introductions\n",
    "# The file I used was captured from the web page on February 5th 2019\n",
    "\n",
    "AUTO_BIOS = 'C:\\\\Users\\\\camer\\\\Downloads\\\\20190205 Topic_ Class Introductions.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Created a custom Class to parse the HTML file and tokenize the document entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "\n",
    "class readClassIntroductions():\n",
    "    \"\"\"\n",
    "    Custom class to read the HTML file containing the class introductions, and tokenize the entries into \n",
    "    lists of lists of token/tag tuples.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fileToRead):\n",
    "        self.char_scrub_list = [\"\\n\",\"â\",\"€\",\"™\",\"\\xa0\",\"/\",'\"',\"(\",\")\",\";\",\":\"]\n",
    "        \n",
    "        soup = BeautifulSoup(open(fileToRead), 'html.parser')\n",
    "        soup_entries = soup.find('ul', class_='discussion-entries')\n",
    "        \n",
    "        self.entries = []\n",
    "        for child in soup_entries.children:\n",
    "            entry = child.find(class_='message').text\n",
    "            entry = self.replace_list(entry, \" \")\n",
    "            self.entries.append(entry)\n",
    "\n",
    "    def replace_list(self, string, replacement=\" \"):\n",
    "        for item in self.char_scrub_list:\n",
    "            string = string.replace(item, replacement)\n",
    "        return string\n",
    "\n",
    "    def tokenize(self):\n",
    "        \"\"\"\n",
    "        Taken and Modified from p. 49 of ATAP textbook\n",
    "        \"\"\"\n",
    "        for entry in self.entries:\n",
    "            yield [\n",
    "                pos_tag(wordpunct_tokenize(sent))\n",
    "                for sent in sent_tokenize(entry)\n",
    "            ]\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two additional Classes are needed to construct the Sklearn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Taken and Modified from pp. 72-73 of ATAP textbook\n",
    "    \"\"\"\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords = set(stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    def is_punctuation(self, token):\n",
    "        return all(unicodedata.category(char).startswith('P') for char in token)\n",
    "    \n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "    \n",
    "    def normalize(self, entry):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for sentence in entry\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punctuation(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "    \n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "    \n",
    "    def fit(self, entries):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, entries):\n",
    "        for entry in entries:\n",
    "            yield self.normalize(entry)\n",
    "            \n",
    "            \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def identity(words):\n",
    "    \"\"\"\n",
    "    Taken from p. 90 of ATAP textbook\n",
    "    An identity function is a function that returns its arguments.\n",
    "    \"\"\"\n",
    "    return words\n",
    "\n",
    "\n",
    "class SklearnTopicModels(object):\n",
    "    \"\"\"\n",
    "    Taken and Modified from pp. 112, 113 and 120 of ATAP textbook\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_topics=5, estimator='LDA'):\n",
    "        \"\"\"\n",
    "        n_topics: the number of topics to identify\n",
    "        estimator: this class will support LDA or LSA analysis (default to LDA)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_topics = n_topics\n",
    "        \n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = TruncatedSVD(n_components=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = LatentDirichletAllocation(n_components=self.n_topics, learning_method='batch')\n",
    "            \n",
    "        self.model = Pipeline([\n",
    "            ('normalize', TextNormalizer()),\n",
    "            ('vectorizer', CountVectorizer(tokenizer=identity, preprocessor=None, lowercase=False)),\n",
    "            ('model', self.estimator)\n",
    "        ])\n",
    "        \n",
    "    def fit_transform(self, entries):\n",
    "        self.model.fit_transform(entries)\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def get_topics(self, n=10):\n",
    "        \"\"\"\n",
    "        n is the number of top terms to show for each topic\n",
    "        \"\"\"\n",
    "        \n",
    "        vectorizer = self.model.named_steps['vectorizer']\n",
    "        model = self.model.steps[-1][1]\n",
    "        names = vectorizer.get_feature_names()\n",
    "        topics = dict()\n",
    "        \n",
    "        for idx, topic in enumerate(model.components_):\n",
    "            features = topic.argsort()[:-(n-1):-1]\n",
    "            tokens = [names[i] for i in features]\n",
    "            topics[idx] = tokens\n",
    "            \n",
    "        return topics\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below program logic calls the Classes above\n",
    "\n",
    "It mimics the:\n",
    "\n",
    "`if __name__ == '__main__':`\n",
    "\n",
    "found in a .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "entries = list(readClassIntroductions(AUTO_BIOS).tokenize())\n",
    "\n",
    "lda = SklearnTopicModels(5,'LDA')\n",
    "lsa = SklearnTopicModels(5,'LSA')\n",
    "\n",
    "lda_start = time()\n",
    "lda.fit_transform(entries)\n",
    "lda_time = time() - lda_start\n",
    "\n",
    "lsa_start = time()\n",
    "lsa.fit_transform(entries)\n",
    "lsa_time = time() - lsa_start\n",
    "\n",
    "\n",
    "print(f\"Time to fit and transform using LDA: {lda_time:06.6f} seconds\\n\")\n",
    "lda_topics = lda.get_topics()\n",
    "for topic, terms in lda_topics.items():\n",
    "    print(f\"LDA Topic #{topic+1}::\")\n",
    "    print(terms)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(f\"Time to fit and transform using LSA: {lsa_time:06.6f} seconds\\n\")\n",
    "lsa_topics = lsa.get_topics()\n",
    "for topic, terms in lsa_topics.items():\n",
    "    print(f\"LSA Topic #{topic+1}::\")\n",
    "    print(terms)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results obtained when re-running the last code cell after initial run of the notebook\n",
    "\n",
    "Time to fit and transform using LDA: 0.045001 seconds\n",
    "\n",
    "LDA Topic #1::  \n",
    "['new', 'work', 'use', 'hello', 'python', 'forward', 'area', 'look']  \n",
    "LDA Topic #2::  \n",
    "['certificate', 'work', 'course', 'take', 'everyone', 'hello', 'name', 'since']  \n",
    "LDA Topic #3::  \n",
    "['machine', 'learning', 'currently', 'data', 'work', 'complete', 'would', 'course']  \n",
    "LDA Topic #4::  \n",
    "['work', 'take', 'live', 'currently', 'hi', 'year', 'class', 'background']  \n",
    "LDA Topic #5::  \n",
    "['learn', 'learning', 'machine', 'data', 'deep', 'certification', 'year', 'class']  \n",
    "\n",
    "\n",
    "\n",
    "Time to fit and transform using LSA: 0.008966 seconds\n",
    "\n",
    "LSA Topic #1::  \n",
    "['work', 'take', 'machine', 'course', 'learning', 'currently', 'data', 'everyone']  \n",
    "LSA Topic #2::  \n",
    "['linguistics', 'background', 'take', 'academic', 'language', 'productivity', 'empower', 'coaching']  \n",
    "LSA Topic #3::  \n",
    "['year', 'learn', 'business', 'enrol', 'space', 'interest', 'intelligence', 'analytics']  \n",
    "LSA Topic #4::  \n",
    "['sentiment', 'data', 'context', 'everyone', 'course', 'complete', 'project', 'work']  \n",
    "LSA Topic #5::  \n",
    "['medium', 'social', 'mention', 'carolina', 'would', 'apply', 'background', 'part']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results obtained when running this notebook with Run All command\n",
    "\n",
    "Time to fit and transform using LDA: 1.415000 seconds\n",
    "\n",
    "LDA Topic #1::  \n",
    "['field', 'text', 'new', 'hello', 'complete', 'interest', 'intelligence', 'take']  \n",
    "LDA Topic #2::  \n",
    "['analytics', 'data', 'work', 'learn', 'class', 'intelligence', 'interest', 'would']  \n",
    "LDA Topic #3::  \n",
    "['work', 'machine', 'learning', 'learn', 'class', 'year', 'hi', 'name']  \n",
    "LDA Topic #4::  \n",
    "['take', 'course', 'certificate', 'machine', 'learning', 'work', 'background', 'everyone']  \n",
    "LDA Topic #5::  \n",
    "['data', 'course', 'certification', 'machine', 'work', 'learning', 'complete', 'hello']  \n",
    "\n",
    "\n",
    "\n",
    "Time to fit and transform using LSA: 0.009966 seconds\n",
    "\n",
    "LSA Topic #1::  \n",
    "['work', 'take', 'machine', 'course', 'learning', 'currently', 'data', 'everyone']  \n",
    "LSA Topic #2::  \n",
    "['linguistics', 'background', 'take', 'language', 'productivity', 'empower', 'academic', 'coaching']  \n",
    "LSA Topic #3::  \n",
    "['year', 'learn', 'business', 'enrol', 'space', 'intelligence', 'interest', 'analytics']  \n",
    "LSA Topic #4::  \n",
    "['sentiment', 'data', 'context', 'everyone', 'course', 'complete', 'project', 'work']  \n",
    "LSA Topic #5::  \n",
    "['mention', 'medium', 'carolina', 'social', 'would', 'apply', 'background', 'part']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
