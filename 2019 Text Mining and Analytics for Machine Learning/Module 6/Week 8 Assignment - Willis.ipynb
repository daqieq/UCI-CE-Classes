{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment and Emotion Analysis of Twitter Feeds - Week 8 Assignment\n",
    "\n",
    "I picked the Monero crypto-currency as a topic, and specifically tried to target the Hard Fork that occured on March 9th, 2019. This was a contencious period and I expected to see a lot of emotion in tweets.\n",
    "\n",
    "Challenges:\n",
    "\n",
    "1) The crypto-currency topic on twitter draws a lot of 'spam' tweets that made it difficult to focus on my chosen topic. If I was doing this again, I would spend more time identifying and removing accounts that create spam.  \n",
    "2) The Tweepy API only allowed me to get tweets for the previous week. Ideally, I would have data back into February 2019 to capture more of the lead up to the March 9th date. In the future, I would plan ahead better, or pick a topic that wasn't time constrained.  \n",
    "3) The Twitter API gave me access to the last 30 days of tweets, but it started with the most recent date and only allowed me to pull 18,000 tweets (since I wasted 6,000+ of my monthly quota testing and debugging). In the future, I would have a better tested starting point, or pay for higher quota limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from searchtweets import ResultStream, gen_rule_payload, load_credentials\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "from datetime import date\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare the Twitter and Tweepy APIs for getting Tweets\n",
    "\n",
    "Each API class follows the same WORM (Write Once Read Many) philosophy and immediately writes the data from the API to a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterAPI(object): \n",
    "    def __init__(self, query):\n",
    "        \n",
    "        try:\n",
    "            self.raw_path = os.path.join(os.getcwd(), 'Searchtweets Raw')\n",
    "            if not os.path.exists(self.raw_path):\n",
    "                os.makedirs(self.raw_path)\n",
    "        except Exception as e:\n",
    "            print(\"Error setting the path for Raw Pickles\\n\" + str(e))\n",
    "            \n",
    "        # Get creds from yaml file in cwd (yaml file not provided in assignment submission)\n",
    "        try:\n",
    "            self.premium_search_args = load_credentials(\"twitter_keys.yaml\",\n",
    "                                       yaml_key=\"search_tweets_api\",\n",
    "                                       env_overwrite=False)\n",
    "        except Exception as e:\n",
    "            print(\"Error setting the search arguments\\n\" + str(e))\n",
    "            \n",
    "        # Create the rule to use in the call\n",
    "        try:\n",
    "            self.rule = gen_rule_payload(query, results_per_call=100) # sandbox accounts limited to 100\n",
    "        except Exception as e:\n",
    "            print(\"Error setting the query rule logic\\n\" + str(e))\n",
    "            \n",
    "        # Create the stream object that will pull the tweets\n",
    "        try:\n",
    "            self.rs = ResultStream(rule_payload=self.rule,\n",
    "                      max_results=18000,\n",
    "                      max_pages=1000,\n",
    "                      **self.premium_search_args)\n",
    "        except Exception as e:\n",
    "            print(\"Error setting the result stream logic\\n\" + str(e))\n",
    "                \n",
    "    def get_tweets(self):\n",
    "        \n",
    "        file_label = date.today().isoformat().replace(\"-\",\"_\") + '_'\n",
    "        \n",
    "        tweets = []\n",
    "        \n",
    "        # Use the Twitter result stream object to bring tweets in\n",
    "        for i, tweet in enumerate(self.rs.stream(), 1):\n",
    "            \n",
    "            tweets.append(tweet)\n",
    "            \n",
    "            # Every 1000 tweets, we need to save them off to a pickle and pause so that we don't exceed rate limits\n",
    "            if i % 1000 == 0:\n",
    "                \n",
    "                # only 10 calls per second allowed - 10 calls * 100 tweets per call = 1000 tweets\n",
    "                # only 30 calls per minute allowed - 30 calls * 100 tweets per call = 3000 tweets\n",
    "                # to be safe ... we will use 1000 every 30 seconds\n",
    "                time.sleep(30)\n",
    "\n",
    "                file_name = file_label + str(i) + '.pickle'\n",
    "                with open(os.path.join(self.raw_path, file_name), 'wb') as f:\n",
    "                    pickle.dump(tweets, f, pickle.HIGHEST_PROTOCOL)\n",
    "                    \n",
    "                tweets.clear()\n",
    "        \n",
    "        # Capture any left over tweets that didn't make it to 1000\n",
    "        if len(tweets) > 0:\n",
    "\n",
    "            file_name = file_label + str(i) + '.pickle'\n",
    "            with open(os.path.join(self.raw_path, file_name), 'wb') as f:\n",
    "                pickle.dump(tweets, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                \n",
    "class TweepyAPI(object): \n",
    "    def __init__(self):\n",
    "        # Provide the Twitter developer creds here\n",
    "        consumer_key = 'xxxx'\n",
    "        consumer_secret = 'xxxx'\n",
    "        access_token = 'xxxx'\n",
    "        access_token_secret = 'xxxx'\n",
    "        \n",
    "        try:\n",
    "            self.raw_path = os.path.join(os.getcwd(), 'Tweepy Raw')\n",
    "            if not os.path.exists(self.raw_path):\n",
    "                os.makedirs(self.raw_path)\n",
    "        except Exception as e:\n",
    "            print(\"Error setting the path for Raw Pickles\\n\" + str(e))\n",
    "            \n",
    "        # Set up the Tweepy API connection, including the rate limit handling\n",
    "        try:\n",
    "            auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "            auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "            self.api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "        \n",
    "        except tweepy.TweepError as e:\n",
    "            print(f\"Error: Twitter Authentication Failed - \\n{str(e)}\")\n",
    "            \n",
    "    # This is the 'fall back' rate limit handler that should not be needed anymore\n",
    "    def _limit_handled(self, cursor):\n",
    "        while True:\n",
    "            try:\n",
    "                yield cursor.next()\n",
    "            except tweepy.RateLimitError:\n",
    "                time.sleep(15 * 60)\n",
    "                continue\n",
    "            except tweepy.TweepError as e:  \n",
    "                print(e.reason)\n",
    "                time.sleep(15 * 60)\n",
    "                continue\n",
    "            except StopIteration:\n",
    "                break\n",
    "                \n",
    "    def get_tweets(self, query, si, un):\n",
    "        \n",
    "        file_label = date.today().isoformat().replace(\"-\",\"_\") + '_' + si.replace(\"-\",\"_\") + \"_\" + un.replace(\"-\",\"_\") + \"_\"\n",
    "\n",
    "        # Loop through the Tweepy cursor one 'page' at a time and save each page to a pickle for processing\n",
    "        for i, page in enumerate(self._limit_handled(tweepy.Cursor(self.api.search, q=query, since=si, until=un, \\\n",
    "                                                                   lang='en', tweet_mode='extended').pages()), 1):\n",
    "\n",
    "            file_name = file_label + str(i) + '.pickle'\n",
    "            with open(os.path.join(self.raw_path, file_name), 'wb') as f:\n",
    "                pickle.dump(page, f, pickle.HIGHEST_PROTOCOL)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the API\n",
    "\n",
    "These are the queries that were used throughout the 2 week period to pull tweets from the 2 API classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grabbing bearer token from OAUTH\n"
     ]
    }
   ],
   "source": [
    "XMR_QUERY = '(XMR OR monero OR #xmr OR #monero OR @monero) lang:en'\n",
    "\n",
    "twit_api = TwitterAPI(XMR_QUERY)\n",
    "\n",
    "# Only run this when needed because it will count against our count of requests (250 per month)\n",
    "# twit_api.get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "XMR_QUERY = '$XMR OR #xmr OR #monero OR @monero'\n",
    "\n",
    "twep_api = TweepyAPI()\n",
    "\n",
    "# Only run this when needed\n",
    "# twep_api.get_tweets(XMR_QUERY,'2019-03-17','2019-03-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Process the Tweets data to prepare for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PreProcess(object):\n",
    "    def __init__(self):\n",
    "        # Paths to the pickles saved from the Twitter and Tweepy APIs\n",
    "        self.tweepy_path = os.path.join(os.getcwd(), 'Tweepy Raw')\n",
    "        self.searchtweets_path = os.path.join(os.getcwd(), 'Searchtweets Raw')\n",
    "        \n",
    "        # New path to save the Train and Test data\n",
    "        self.pre_path = os.path.join(os.getcwd(), 'PreProcess Pickles')\n",
    "    \n",
    "    # Regex helper function\n",
    "    def _remove_pattern(self, text, pattern_regex):\n",
    "        r = re.findall(pattern_regex, text)\n",
    "        for i in r:\n",
    "            text = re.sub(i, '', text)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    # Read the Tweepy tweets\n",
    "    def _read_tweepy_pickles(self):\n",
    "        for filename in os.listdir(self.tweepy_path):\n",
    "            if '.pickle' in filename:\n",
    "                with open(os.path.join(self.tweepy_path, filename), 'rb') as f:\n",
    "                    yield pickle.load(f)\n",
    "\n",
    "    # Read the Twitter tweets\n",
    "    def _read_searchtweets_pickles(self):\n",
    "        for filename in os.listdir(self.searchtweets_path):\n",
    "            if '.pickle' in filename:\n",
    "                with open(os.path.join(self.searchtweets_path, filename), 'rb') as f:\n",
    "                    yield pickle.load(f)\n",
    "    \n",
    "    def _getText(self, data):\n",
    "        # From github issue # 878: https://github.com/tweepy/tweepy/issues/878\n",
    "        # Try for extended text of original tweet, if RT'd (streamer)\n",
    "        try: text = data['retweeted_status']['extended_tweet']['full_text']\n",
    "        except: \n",
    "            # Try for extended text of an original tweet, if RT'd (REST API)\n",
    "            try: text = data['retweeted_status']['full_text']\n",
    "            except:\n",
    "                # Try for extended text of an original tweet (streamer)\n",
    "                try: text = data['extended_tweet']['full_text']\n",
    "                except:\n",
    "                    # Try for extended text of an original tweet (REST API)\n",
    "                    try: text = data['full_text']\n",
    "                    except:\n",
    "                        # Try for basic text of original tweet if RT'd \n",
    "                        try: text = data['retweeted_status']['text']\n",
    "                        except:\n",
    "                            # Try for basic text of an original tweet\n",
    "                            try: text = data['text']\n",
    "                            except: \n",
    "                                # Nothing left to check for\n",
    "                                text = ''\n",
    "        return text\n",
    "    \n",
    "    # This function was my one attempt to remove spam Tweets\n",
    "    def get_exclude_list(self):\n",
    "        \n",
    "        dd_users = defaultdict(int)\n",
    "        \n",
    "        for tweets in self._read_searchtweets_pickles():\n",
    "            for tweet in tweets:\n",
    "                dd_users[tweet['user']['screen_name']] += 1\n",
    "\n",
    "        for page in self._read_tweepy_pickles():\n",
    "            for status in page:\n",
    "                dd_users[status.user.screen_name] += 1\n",
    "            \n",
    "        df_list = pd.Series(dd_users).to_frame('Count')\n",
    "        \n",
    "        df_sorted = df_list.sort_values('Count', axis=0, ascending=False)\n",
    "\n",
    "        # Exclude top 7 tweeters because they are posting repetitive 'spam' posts\n",
    "        return df_sorted.head(7).index.values.tolist()\n",
    "            \n",
    "    def _removeUnneededText(self, tweet):\n",
    "        \n",
    "        # Remove user Twitter handles and the 'RT' word\n",
    "        tweet = self._remove_pattern(tweet, r'(@[\\w]*\\s|@[\\w]*:\\s|\\bRT\\s)')\n",
    "        \n",
    "        # Remove any http links\n",
    "        tweet_words = [word for word in tweet.split() if 'http' not in word]\n",
    "        tweet = ' '.join(tweet_words)\n",
    "        \n",
    "        return tweet\n",
    "                \n",
    "    def process_raw_pickles(self):\n",
    "        tweet_list = []\n",
    "        \n",
    "        # Get a list of the top 7 users, because we will assume that they are spammers\n",
    "        exclude = self.get_exclude_list()\n",
    "\n",
    "        # Read Tweepy tweets and add them to list\n",
    "        for page in self._read_tweepy_pickles():\n",
    "            for status in page:\n",
    "                if status.user.screen_name not in exclude:\n",
    "                    text = self._getText(status._json)\n",
    "                    text = self._removeUnneededText(status.full_text)\n",
    "\n",
    "                    # Add to list if it's not a duplicate\n",
    "                    if status.retweet_count > 0: \n",
    "                        if text not in tweet_list:\n",
    "                            tweet_list.append(text)\n",
    "                    else:\n",
    "                        tweet_list.append(text)\n",
    "    \n",
    "        # Read Twitter tweets and add them to list\n",
    "        for tweets in self._read_searchtweets_pickles():\n",
    "            for tweet in tweets:\n",
    "                if tweet['user']['screen_name'] not in exclude:\n",
    "                    text = self._getText(tweet)\n",
    "                    text = self._removeUnneededText(text)\n",
    "\n",
    "                    # Add to list if it's not a duplicate\n",
    "                    if tweet.retweet_count > 0: \n",
    "                        if text not in tweet_list:\n",
    "                            tweet_list.append(text)\n",
    "                    else:\n",
    "                        tweet_list.append(text)\n",
    "                    \n",
    "        # Do one more step to eliminate duplicate tweets\n",
    "        df_tweets = pd.DataFrame(tweet_list, columns=['tweets']).drop_duplicates()\n",
    "        \n",
    "        # Set this parameter manually to get over 1000 tweets to use for Training\n",
    "        msk = np.random.rand(len(df_tweets)) < 0.111\n",
    "\n",
    "        # Use the boolean mask array to split the data into Train and Test datasets\n",
    "        train_df = df_tweets[msk]\n",
    "        test_df = df_tweets[~msk]\n",
    "        \n",
    "        # Save TRAIN data to CSV for manual classication in Excel\n",
    "        train_df.to_csv(os.path.join(self.pre_path, 'Train_data.csv'), encoding='utf-8', index=False)\n",
    "        \n",
    "        # Save TEST data to pickle for later use\n",
    "        with open(os.path.join(self.pre_path, 'Test_data.pickle'), 'wb') as f:\n",
    "            pickle.dump(test_df, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        # Return stats for user to review\n",
    "        return len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the pre process class and the function to split the data into Train and Test\n",
    "PP = PreProcess()\n",
    "\n",
    "# At this point, the data is split and we don't want to rerun this step again because it will write over our data\n",
    "#    that we are using for Training\n",
    "# train_test_lengths = PP.process_raw_pickles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Classification of Train Data\n",
    "\n",
    "The CSV output from the above PreProcess class was backed up in case it was erased or lost. Then I manually tagged each tweet for the five emotions, with respect to my target event:\n",
    "\n",
    "    Anger, Enthusiasm, Passivity, Fear and Hope\n",
    "\n",
    "Once that was complete, I saved the results back to a new CSV file that will be read by the next class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test\n",
    "\n",
    "Based on instructions in the discussion group, I used a simple dictionary counting classifier. However, I tried to exended it a little bit by writting 4 tests instead of just relying on highest count. This offsets the fact that the 'passivity' emotion was highly represented in the tweet training data.\n",
    "\n",
    "In future projects, I would use documented TF-IDF, Word2Vec or another method to vectorize my data first. And then use either ML classifiers with Sklearn pipelines to test best classifier, or use a Recurrent Neural Network such as LSTM or GRU.\n",
    "\n",
    "For this project, it was a good exercise to see how the data was constructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basic_Train_Test(object):\n",
    "    def __init__(self):\n",
    "        # Read from the PreProcess path\n",
    "        self.pre_path = os.path.join(os.getcwd(), 'PreProcess Pickles')\n",
    "        \n",
    "        # Dicts for recording words used in Training tweets\n",
    "        self.dict_an = defaultdict(int)\n",
    "        self.dict_en = defaultdict(int)\n",
    "        self.dict_pa = defaultdict(int)\n",
    "        self.dict_fe = defaultdict(int)\n",
    "        self.dict_ho = defaultdict(int)\n",
    "\n",
    "        # Totals to record the total number of words used in each emotion category\n",
    "        self.an_ttl = self.en_ttl = self.pa_ttl = self.fe_ttl = self.ho_ttl = 0\n",
    "        \n",
    "        # Dict of emotion categories to use when labelling Test data\n",
    "        self.emotions = {0: \"anger\", 1: \"enthusiasm\", 2: \"passivity\", 3: \"fear\", 4: \"hope\"}\n",
    "    \n",
    "    def train_data(self):\n",
    "        # Read the manually tagged CSV data\n",
    "        df_train = pd.read_csv(os.path.join(self.pre_path, 'Train_data_Classified.csv'))\n",
    "        \n",
    "        train_ = df_train.values.tolist()\n",
    "        \n",
    "        for tweet in train_:\n",
    "            # Clean the text and remove stop words\n",
    "            temp = re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet[0]).lower()\n",
    "            words = [word for word in temp.split() if word not in stopwords.words('english')]\n",
    "\n",
    "            # Put words in dicts based on their category and count them\n",
    "            for word in words:\n",
    "                if tweet[1] == \"anger\":\n",
    "                    self.an_ttl += 1\n",
    "                    if word in self.dict_an:\n",
    "                        self.dict_an[word] += 1\n",
    "                    else:\n",
    "                        self.dict_an[word] = 1\n",
    "\n",
    "                elif tweet[1] == \"enthusiasm\":\n",
    "                    self.en_ttl += 1\n",
    "                    if word in self.dict_en:\n",
    "                        self.dict_en[word] += 1\n",
    "                    else:\n",
    "                        self.dict_en[word] = 1\n",
    "\n",
    "                elif tweet[1] == \"passivity\":\n",
    "                    self.pa_ttl += 1\n",
    "                    if word in self.dict_pa:\n",
    "                        self.dict_pa[word] += 1\n",
    "                    else:\n",
    "                        self.dict_pa[word] = 1\n",
    "\n",
    "                elif tweet[1] == \"fear\":\n",
    "                    self.fe_ttl += 1\n",
    "                    if word in self.dict_fe:\n",
    "                        self.dict_fe[word] += 1\n",
    "                    else:\n",
    "                        self.dict_fe[word] = 1\n",
    "\n",
    "                elif tweet[1] == \"hope\":\n",
    "                    self.ho_ttl += 1\n",
    "                    if word in self.dict_ho:\n",
    "                        self.dict_ho[word] += 1\n",
    "                    else:\n",
    "                        self.dict_ho[word] = 1\n",
    "\n",
    "                else:\n",
    "                    raise ValueError\n",
    "        \n",
    "        # Return stats for the user to review\n",
    "        return { \"anger\": (len(self.dict_an), self.an_ttl), \"enthusiasm\": (len(self.dict_en), self.en_ttl), \\\n",
    "                 \"passivity\": (len(self.dict_pa), self.pa_ttl), \"fear\": (len(self.dict_fe), self.fe_ttl), \\\n",
    "                 \"hope\": (len(self.dict_ho), self.ho_ttl) }\n",
    "    \n",
    "    def test_data(self):\n",
    "        # Read back test data from pickle\n",
    "        with open(os.path.join(self.pre_path, 'Test_data.pickle'), 'rb') as f:\n",
    "            df_test = pickle.load(f)\n",
    "        \n",
    "        test_ = df_test.values.tolist()\n",
    "        \n",
    "        for tweet in test_:\n",
    "            # Clean the text\n",
    "            temp = re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet[0]).lower()\n",
    "            \n",
    "            an_cnt = en_cnt = pa_cnt = fe_cnt = ho_cnt = 0\n",
    "            \n",
    "            # Count each word based on the dictionary of emtions created while training\n",
    "            for word in temp.split():\n",
    "                # Remove any stop words and then add the counts of the word from the emotion categories\n",
    "                # This creates a cumulative 'weight' of the words in this category from the training data\n",
    "                # Note - This 'weight' needs to be 'offset' by size of each category, i.e. len(dict_*), or \n",
    "                #        by the number of words in each category, i.e. *_ttl. Hence, the matrix of tests.\n",
    "                if word not in stopwords.words('english'):\n",
    "                    \n",
    "                    an_cnt += self.dict_an[word]\n",
    "                    en_cnt += self.dict_en[word]\n",
    "                    pa_cnt += self.dict_pa[word]\n",
    "                    fe_cnt += self.dict_fe[word]\n",
    "                    ho_cnt += self.dict_ho[word]\n",
    "\n",
    "            # Create a matrix of values based on 4 (or more) different statistical tests\n",
    "            mtrx = [\n",
    "                    [an_cnt/len(self.dict_an),\n",
    "                     en_cnt/len(self.dict_en),\n",
    "                     pa_cnt/len(self.dict_pa),\n",
    "                     fe_cnt/len(self.dict_fe),\n",
    "                     ho_cnt/len(self.dict_ho)],\n",
    "                    [math.log(an_cnt+1)/len(self.dict_an),\n",
    "                     math.log(en_cnt+1)/len(self.dict_en),\n",
    "                     math.log(pa_cnt+1)/len(self.dict_pa),\n",
    "                     math.log(fe_cnt+1)/len(self.dict_fe),\n",
    "                     math.log(ho_cnt+1)/len(self.dict_ho)],\n",
    "                    [an_cnt/math.log(len(self.dict_an)),\n",
    "                     en_cnt/math.log(len(self.dict_en)),\n",
    "                     pa_cnt/math.log(len(self.dict_pa)),\n",
    "                     fe_cnt/math.log(len(self.dict_fe)),\n",
    "                     ho_cnt/math.log(len(self.dict_ho))],\n",
    "                    [an_cnt/self.an_ttl,\n",
    "                     en_cnt/self.en_ttl,\n",
    "                     pa_cnt/self.pa_ttl,\n",
    "                     fe_cnt/self.fe_ttl,\n",
    "                     ho_cnt/self.ho_ttl]\n",
    "                   ]\n",
    "\n",
    "            mtrx_scr = []\n",
    "\n",
    "            # Score the matrix from 0-5 based on highest to lowest score\n",
    "            # Highest being the most likely category based on the best word matches\n",
    "            for test in mtrx:\n",
    "                rev = sorted(test, reverse=True)\n",
    "                d = {k: v for v, k in enumerate(rev)}\n",
    "                mtrx_scr.append([d[v] for v in test])\n",
    "\n",
    "            d_scr = defaultdict(int)\n",
    "\n",
    "            # Use a dictionary to sum up the test scores and then compute the average score across all tests\n",
    "            for j in range(len(mtrx_scr[0])):\n",
    "                for i in range(len(mtrx_scr)):\n",
    "                    d_scr[j] += mtrx_scr[i][j]\n",
    "                d_scr[j] /= len(mtrx_scr)\n",
    "\n",
    "            # Yield back the tweet and the emotion category label based on the lowest score\n",
    "            yield [tweet[0], self.emotions[min(d_scr, key=d_scr.get)]]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each emotion label has a tuple of (number of distinct words in the dictionary, total number of words):\n",
      "\n",
      "{'anger': (183, 214), 'enthusiasm': (1152, 2756), 'passivity': (3451, 14668), 'fear': (536, 959), 'hope': (587, 1123)}\n"
     ]
    }
   ],
   "source": [
    "# Call the train test class and then train on the traning CSV data\n",
    "BTT = Basic_Train_Test()\n",
    "train_results = BTT.train_data()\n",
    "print(\"Each emotion label has a tuple of (number of distinct words in the dictionary, total number of words):\\n\")\n",
    "print(train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generator that can be used to pull the tested data down\n",
    "test_results = BTT.test_data()\n",
    "\n",
    "df_tested = pd.DataFrame(list(test_results), columns=['tweets','emotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Testing\n",
    "\n",
    "The counts by category below seem in line with the counts that we saw from training above. However, based on reviewing manually the results, I don't think this classifier did a very good job. That is, I think the accuracy of this method of classifying is very low.\n",
    "\n",
    "I think it will require one of the more advanced classifiers to correctly classify these emotions better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enthusiasm</th>\n",
       "      <td>2362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hope</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passivity</th>\n",
       "      <td>5442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweets\n",
       "emotion           \n",
       "anger           84\n",
       "enthusiasm    2362\n",
       "fear            21\n",
       "hope            19\n",
       "passivity     5442"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tested.groupby('emotion').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples of tweets by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>#XMR Buy at #Bittrex and sell at #Bitfinex. Ra...</td>\n",
       "      <td>passivity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>1/Xmr is the true cypher punk crypto vision.. ...</td>\n",
       "      <td>passivity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6467</th>\n",
       "      <td>Yay! New 4 blocks: 8.24 MSR (133.77%@432420), ...</td>\n",
       "      <td>passivity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>$XMR has now been in in this consolidation ran...</td>\n",
       "      <td>passivity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>ðŸ”„ Prices update in $USD (1 hour): $EOS - 3.69 ...</td>\n",
       "      <td>passivity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets    emotion\n",
       "3068  #XMR Buy at #Bittrex and sell at #Bitfinex. Ra...  passivity\n",
       "5179  1/Xmr is the true cypher punk crypto vision.. ...  passivity\n",
       "6467  Yay! New 4 blocks: 8.24 MSR (133.77%@432420), ...  passivity\n",
       "1284  $XMR has now been in in this consolidation ran...  passivity\n",
       "1910  ðŸ”„ Prices update in $USD (1 hour): $EOS - 3.69 ...  passivity"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tested.loc[df_tested['emotion'] == \"passivity\"].sample(n=5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4518</th>\n",
       "      <td>Monero gold</td>\n",
       "      <td>enthusiasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4729</th>\n",
       "      <td>I've just posted a new blog: Altcoin News: Mon...</td>\n",
       "      <td>enthusiasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7023</th>\n",
       "      <td>360 Total Security team uncovered a new Monero...</td>\n",
       "      <td>enthusiasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>Facebook and Monero?</td>\n",
       "      <td>enthusiasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3573</th>\n",
       "      <td>(FEBRUARY) via /r/Monero hot ðŸ”¥ in #reddit #Mon...</td>\n",
       "      <td>enthusiasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets     emotion\n",
       "4518                                        Monero gold  enthusiasm\n",
       "4729  I've just posted a new blog: Altcoin News: Mon...  enthusiasm\n",
       "7023  360 Total Security team uncovered a new Monero...  enthusiasm\n",
       "6835                               Facebook and Monero?  enthusiasm\n",
       "3573  (FEBRUARY) via /r/Monero hot ðŸ”¥ in #reddit #Mon...  enthusiasm"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tested.loc[df_tested['emotion'] == \"enthusiasm\"].sample(n=5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7593</th>\n",
       "      <td>Unbelievable.</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7811</th>\n",
       "      <td>dictatorship</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>I will sue you too</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7469</th>\n",
       "      <td>Actuales.</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5130</th>\n",
       "      <td>because you're fucking retarded.</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                tweets emotion\n",
       "7593                     Unbelievable.   anger\n",
       "7811                      dictatorship   anger\n",
       "2432                I will sue you too   anger\n",
       "7469                         Actuales.   anger\n",
       "5130  because you're fucking retarded.   anger"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tested.loc[df_tested['emotion'] == \"anger\"].sample(n=5, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6396</th>\n",
       "      <td>I've almost failed to unlock my aunties ipad m...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>He's right about that metadata, at least $RYO ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Ban Anonymous Cryptocurrencies, Says French Na...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7234</th>\n",
       "      <td>Where are points put up? Super coach doesnâ€™t h...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>Should have used @monero</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets emotion\n",
       "6396  I've almost failed to unlock my aunties ipad m...    fear\n",
       "3196  He's right about that metadata, at least $RYO ...    fear\n",
       "269   Ban Anonymous Cryptocurrencies, Says French Na...    fear\n",
       "7234  Where are points put up? Super coach doesnâ€™t h...    fear\n",
       "1105                           Should have used @monero    fear"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tested.loc[df_tested['emotion'] == \"fear\"].sample(n=5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4901</th>\n",
       "      <td>Young Buck saying \"next time, we only using Dr...</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>Ok, yes the signatures are ECDSA, but bullet p...</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7671</th>\n",
       "      <td>im waiting</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>Where is the trezor implementation ?????</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>Question about the RandomX algo</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets emotion\n",
       "4901  Young Buck saying \"next time, we only using Dr...    hope\n",
       "1739  Ok, yes the signatures are ECDSA, but bullet p...    hope\n",
       "7671                                         im waiting    hope\n",
       "685            Where is the trezor implementation ?????    hope\n",
       "4968                    Question about the RandomX algo    hope"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tested.loc[df_tested['emotion'] == \"hope\"].sample(n=5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
